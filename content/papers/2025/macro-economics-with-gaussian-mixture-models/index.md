---
title: "Macro Economics with Gaussian Mixture Models"
summary: "
There's an idea that returns are a product of the current market state or regime. In this article, we will explore a way to model these regimes and inform portfolio construction. We'll also look at how to extend the model to include macro-economic variables tying the regimes to observable economic indicators.
"

date: "2025-11-18"
type: paper
mathjax: true
authors:
    - Adrian Letchford
categories:
    - mathematics
    - finance
---
<note>A while ago I read an article published by Two Sigma on using Gaussian Mixture Models for detecting regimes [^Botte2021]. I wanted to take the time to understand the maths behind these methods and how you might take things one step further and tie the results back to economic variables. This article is the write up of what I did.</note>

One way to think of market returns is that they are generated by some underlying economic state or regime. For example, during a recession equity returns tend to be lower and more volatile than during a period of growth. The recession is one state while the period of growth is another.

The state of the economy is not directly observable, only the returns in the market and various economic indicators. As the state is not directly observable, we call it a *latent* or *hidden* state.

Models that incorporate hidden states are called *hidden state models* or *state space models*. There are a few types, but the one we will focus on here is the finite mixture model. A *finite mixture model* assumes that there are a finite number of hidden states and that each state is associated with a probability distribution that generates the observed data. We will focus on the Gaussian mixture model (GMM) where each state is associated with a multivariate Gaussian distribution.

In this article, we will cover the following:
1. The basics of Gaussian mixture models.
1. The key modelling decisions.
1. Modelling states as dependent on covariates.
1. Performing an economic analysis.

# The basics

## Finite mixture models

We are going to model a market with $d$ assets. The vector of returns at time $t$ is $\boldsymbol{r}_t \in \mathbb{R}^d$. In a hidden state model, we say that the market is in one of $K$ states and that each state is associated with a distribution that generates $\boldsymbol{r}_t$.

There are two notations for the state. We can say that the state is $k$, i.e. we can ask "what is the distribution of $\boldsymbol{r}_t$ given we are in state $k$?". We can also represent the state with a vector $\boldsymbol{z}$ which has $z\_{k} = 1$ if the state is $k$ and $0$ for all other states.

Based on this definition of states, we can name a few distributions without specifying their exact form.

**Prior distribution.** The probability of being in state $k$ is:
$$
p(k)
$$

**Emission distribution.** The probability of observing $\boldsymbol{r}_t$ given we are in state $k$ is:
$$
p(\boldsymbol{r}_t | k)
$$
this is often called the *emission* distribution. Later on, we will give this a shape (Gaussian) and parameters.

**Joint distribution.** The probability that we are in state $k$ *and* we observe $\boldsymbol{r}_t$ is (using both notations):
$$
\begin{aligned}
p(k, \boldsymbol{r}_t) &= p(k) p(\boldsymbol{r}_t | k) \\\
\\\
p(\boldsymbol{z}, \boldsymbol{r}_t) &= \prod_k \left[ p(k) p(\boldsymbol{r}_t | k) \right]^{z_k} \\\
\end{aligned}
$$
The information $\boldsymbol{z}$ and $\boldsymbol{r}_t$ are often called the *full set* because at time $t$ the full set of information is the state and the realised returns.

**Mixture distribution.** The probability that we observe $\boldsymbol{r}_t$ is:
$$
p(\boldsymbol{r}_t) = \sum_k^K p(k)p(\boldsymbol{r}_t | k)
$$
Here we've combined or "mixed" the emission distributions into a "mixture" distribution. Under this model, the prior distribution $p(k)$ is sometimes referred to as the *mixing weights*.

**Posterior distribution.** The probability that we are in state $k$ given we observe $\boldsymbol{r}_t$ is derived from the above distributions using Bayes' theorem:
$$
p(k|\boldsymbol{r}_t) = \frac{p(k)p(\boldsymbol{r}_t|k)}{p(\boldsymbol{r}_t)} = \frac{p(k)p(\boldsymbol{r}_t|k)}{\sum_k^K p(k)p(\boldsymbol{r}_t | k)}
$$
This is often viewed as the *responsibility* that state $k$ has for explaining or generating the observation $\boldsymbol{r}_t$ [^Bishop2006].

To do useful things with this latent state model, we need to assume specific distributions for $p(k)$ and $p(\boldsymbol{r}_t | k)$.

## Mixture of Gaussians

We will fix the priors to a single value:
$$
p(k) = \pi_k, \quad \pi_k \ge 0, \quad \sum \pi_k = 1
$$
The emission distribution is set as a Gaussian:
$$
\begin{aligned}
p(\boldsymbol{r}_t | k) &= \mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \\\
&= \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}_k|^{1/2}} e^{ \left( -\frac{1}{2} (\boldsymbol{r}_t - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\boldsymbol{r}_t - \boldsymbol{\mu}_k) \right)} \\\
\end{aligned}
$$
which means the mixture distribtion is:
$$
p(\boldsymbol{r}_t) = \sum_k^K \pi_k \mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$
which is called *a mixture of Gaussians* [^Bishop2006] or a Gaussian mixture model (GMM).

The posterior distribution becomes:
$$
p(k|\boldsymbol{r}_t) = \frac{\pi_k\mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_k^K \pi_k\mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
$$

{{<figure src="images/1d_gmm_example.svg" title="Example of mixing Gaussians." >}}
Two Guassian distributions are shown on the top panel. By mixing the two we get a new distrituion illustrated on the bottom panel. We see an almost Gaussian looking shape with a pronounced right tail. Mixing more Gaussians would result in more complex shapes.
{{</figure>}}


## Estimation

Define $\boldsymbol{R} = [\boldsymbol{r}_1, \dots, \boldsymbol{r}_t]$ to be all the observations and $\boldsymbol{Z} = [\boldsymbol{z}_1, \dots, \boldsymbol{z}_t]$ to be the corresponding hidden states. We will fix the number of states to be $K$. We want to estimate the values $\pi_k$, $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$ for each of the states.

The usual method of estimating a model's parameters is to maximise the log of the likelihood function for the given set of samples. In this case, the likelihood is:
$$
p(\boldsymbol{R}) = \prod_t p(\boldsymbol{r}_t) = \prod_t \left[ \sum_k^K p(k)p(\boldsymbol{r}_t | k) \right]
$$
Taking the log gives us this:
$$
\begin{aligned}
\mathcal{L}( \boldsymbol{R} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) &= \sum_t \log \left( \sum_k^K p(k)p(\boldsymbol{r}_t | k) \right) \\\
&= \sum_t \log \left( \sum_k^K \pi_k \mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right) \\\
\end{aligned}
$$
which is a very complex function to maximise due to the summation within the logarithm. In fact, there isn't a closed form solution to this. We can derive an iterative solution which is called the expectation maximisation (EM) algorithm in general. The EM algorithm is an iterative method of finding a solution to a hidden state model. There is a fair amount of theory which you can read about [^Bishop2006]. Here, we will just give a light derivation.

We first switch from using $p(\boldsymbol{R})$ to using the full set distribution and its log-likelihood:
$$
\begin{aligned}
p(\boldsymbol{Z}, \boldsymbol{R}) &= \prod_t\prod_k \left[ p(k) p(\boldsymbol{r}_t | k) \right]^{z\_{tk}} \\\
&= \prod_t\prod_k \left[ \pi_k \mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right]^{z\_{tk}} \\\
\mathcal{L}( \boldsymbol{R}, \boldsymbol{Z} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) &= \\sum_t \sum_k z\_{tk} \log \left( \pi_k \mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right) \\\
\end{aligned}
$$

The EM algorithm proceeds as follows:
1. **Initialise**: Pick some initial values for the parameters $\pi_k$, $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$.
2. **E-step**: Calculate the expected value of $\boldsymbol{Z}$ given: $\boldsymbol{R}$, $\pi_k$, $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$ $\forall k$.
3. **M-step**: Use these values as values for $\boldsymbol{Z}$ and maximise the log-likelihood with respect to the parameters $\pi_k$, $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$.
4. **Check**: Calculate the log-likelihood $\mathcal{L}( \boldsymbol{R} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})$ and check for convergence in the parameters or the log-likelihood value. If not converged, return to step 2.

Initialisation involves picking some starting value for the posterior probabilities. Once you have those, you can proceed with the M-step to complete the initialisation. These starting probabilities can either be random or based on K-means clustering. For the clustering method, K-means is run and the closest cluster centre sets the initial probability to 1 for that state and 0 for all other states. K-means assumes the clusters are spherical (no correlations). This is not appropriate for financial returns, so we will use random initialisation.

**E-step** Recall that each element of $\boldsymbol{Z}$ is a one-hot vector indicating the state at time $t$. The expected value of $z\_{tk}$ is just the posterior probability that we are in state $k$ given $\boldsymbol{r}_t$:
$$
E[z\_{tk}] = p(k | \boldsymbol{r}_t) = \frac{\pi_k\mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_k^K \pi_k\mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
$$

**M-step** To maximise the log-likelihood with respect to the parameters, we take the derivatives and set them to zero. This gives us the following update equations:
$$
\begin{aligned}
\pi_k &= \frac{T_k}{T} \\\
\boldsymbol{\mu}_k &= \frac{1}{T_k}\sum_t E[z\_{tk}] \boldsymbol{r}_t \\\
\boldsymbol{\Sigma}_k &= \frac{1}{T_k} \sum_t E[z\_{tk}] (\boldsymbol{r}_t - \boldsymbol{\mu}_k)(\boldsymbol{r}_t - \boldsymbol{\mu}_k)^\top
\end{aligned}
$$
where
$$
T_k = \sum_t E[z\_{tk}]
$$
is the number of samples attributed to state $k$.

## Portfolio construction

For each of the $k$ states, if we have portfolio weights $\boldsymbol{w}$ we can calculate the expected portfolio return as:
$$
E_k[r_t] = \mu_k = \boldsymbol{w}^\top \boldsymbol{\mu}_k
$$
and the expected variance as:
$$
\sigma^2_k = \boldsymbol{w}^\top \boldsymbol{\Sigma}_k \boldsymbol{w}
$$
We can safely assume that the portfolio return between each state is uncorrelated.


Given a set of PDFs $f_i$ and mixing weights $p_i$ the PDF of the mixture is:
$$
f(x) = \sum_i p_i f_i(x)
$$
It follows that any moment of the mixture distribution is the weighted sum of the moments of the individual distributions:
$$
E[X^n] = \sum_i p_i E_i[X^n]
$$

Thus, the expected portfolio return across all states is:
$$
E[r_t] = \mu = \sum_k^K \pi_k E_k[r_t] = \sum_k^K \pi_k \boldsymbol{w}^\top\boldsymbol{\mu}_k
$$
We can collect that last sum on the right hand side into a single mean vector:
$$
\boldsymbol{\mu} = \sum_k^K \pi_k \boldsymbol{\mu}_k
$$
such that the expected portfolio return is:
$$
E[r_t] = \mu = \boldsymbol{w}^\top \boldsymbol{\mu}
$$

The expected portfolio variance across all states is:
$$\begin{aligned}
E[(r_t - E[r_t])^2] &= E[r_t^2] - (E[r_t])^2 \\\
&= \sum_k^K \pi_k E_k[r_t^2] - (E[r_t])^2 \\\
&= \sum_k^K \pi_k \left( \sigma^2_k + (E_k[r_t])^2 \right) - (E[r_t])^2 \\\
&= \sum_k^K \pi_k \sigma^2_k + \sum_k^K \pi_k (E_k[r_t])^2 - (E[r_t])^2 \\\
\end{aligned}
$$
Expanding out the variance and moments:
$$
= \sum_k^K \pi_k \boldsymbol{w}^\top \boldsymbol{\Sigma}_k \boldsymbol{w} + \sum_k^K \pi_k (\boldsymbol{w}^\top \boldsymbol{\mu}_k)^2 - ( \boldsymbol{w}^\top \boldsymbol{\mu} )^2
$$
which is just a sum of quadratic terms. We can collect the terms into a single covariance matrix:
$$
\boldsymbol{\Sigma} = \sum_k^K \pi_k \boldsymbol{\Sigma}_k + \sum_k^K \pi_k \boldsymbol{\mu}_k \boldsymbol{\mu}_k^\top - \boldsymbol{\mu}\boldsymbol{\mu}^\top
$$
such that the expected portfolio variance is:
$$
\sigma^2 = \boldsymbol{w}^\top \boldsymbol{\Sigma} \boldsymbol{w}
$$

In practice, you'll find that the GMM mean and cov matches almost exactly the empirical mean and cov when fitted to financial returns. The only difference is that in the GMM model, we can break the mean and cov down into different states. Later, we will modify the prior distribution (the mixing weights) to get time-varying means and covariances.

# Modelling decisions

Before we dig into an example, there are a few key modelling decisions to make. We need to decide what **time interval** to use for returns, whether or not to **devolatise** and **how many states** to use. We'll look at each of these in turn.

As we go through each of these decisions, we'll work on real data. Since we're interested in macro-economic regimes, we'll use broad market ETFs that represent different asset classes. Specifically, we'll use:
- SPY -- *US equities*
- TLT -- *US long-term bonds*
- GLD -- *Gold*
- GSG -- *Commodities*

## Time period

Now, we want to try and tie market regimes to macro-economic variables. These macro-economic variables are usually released at monthly or quarterly intervals. Quarterly intervals cuts the number of samples by a third. As such, we'll use monthly intervals for the economic variables and market returns.

The ETFs that we're using do not have data going back very far. To get around this, we splice in index data for SPY, GLD and GSG. For TLT we use bond yields to approximate long-term bond returns before TLT's inception. This gets us prices going back to 1990. The details are in the appendix. 

The dataset of prices is available [here](broad_etf_prices.csv).

You can load the prices with:
```python
import pandas as pd
daily = pd.read_csv('broad_etf_prices.csv', index_col=0, parse_dates=True)
monthly = daily.resample('MS').last()
```
There are 419 months in the sample which looks like this:

![](images/asset_returns.svg)


## Devolatise

Returns are [heteroscedastic](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity), meaning that their volatility changes over time. This causes problems for machine learning models which generally assume that the data is identically distributed. For GMMs, they will end up fitting states to different volatility regimes rather than different economic regimes.

We can avoid this by normalising the returns by their recent volatility. This procedure is referred to as *devolatising* returns. A simple way to do this is to divide the returns by their exponentially weighted moving standard deviation.

We can see how big of an impact this procedure has by testing on SPY returns. We will do this on daily returns as it is easier to visualise.

```python
# Get the daily returns
returns = daily['SPY'].pct_change()

# Standardize the returns so that the result
# of all operations are on the same scale.
returns -= returns.mean()
returns /= returns.std()

# Calculate the rolling volatility before
# normalising over the last six months.
before = returns.ewm(halflife=126, min_periods=21).std()

# Remove the volatility clustering. Shift by 1
# so that this could be used in a live trading
# scenario.
std = returns.ewm(halflife=21, min_periods=21).std()
std = std.shift(1)
returns /= std

# Calculate the rolling volatility after
# normalising over the last six months.
after = returns.ewm(halflife=126, min_periods=21).std()
```

Plotting `before` and `after` gives us:

{{<figure src="images/volatility_normalization.svg" title="Example of devolatising returns." >}}
The exponentially weighted standard deviation of SPY daily returns are shown in blue. A half-life of 6 months was used. The SPY returns were then devolatised by dividing by the one month exponentially weighted standard deviation lagged by 1 to reflect real world trading. The results are shown in orange. We can see that the big swings in volatility have been removed.
{{</figure>}}

From here on, we will devolatise the monthly returns before fitting a GMM. We will devolatise with a 6 month half-life. This is a hyperparameter that could be tuned. However, this article is focused on the GMM rather than devolatisation so we will not go into detail here.

## Number of states

The number of states is a critical parameter as we want each state to represent a meaningful economic regime. If we have too few states, then we will end up mixing together different regimes (underfitting). If we have too many states, then we will end up with states that are not economically meaningful (overfitting).

We will do a quick experiment to select the number of states. As a warning, this is not a rigorous method as we will be looking at out-of-sample results. But, it will give us a good idea of the number of states that we can work with for the rest of the article.

The experiment:
1. Split the data into a training set and a test set. Training set is everything before 2022-01-01 and test set is everything after. The test set has about 11% of the data.
1. Loop over a range for the number of states (1 to 8).
1. For each number of states, do 100 fits where we fit a GMM to the training set. The GMM does 40 random initialisations and picks the best one.
1. For each fit save the number of states and the test set log-likelihood.

We can fit a Gaussian Mixture Model with the help of scikit-learn's `GaussianMixture` class ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)).

Once we have all the results, we can plot the median test set log-likelihood for each number of states. We'll pick the number of states that maximises the median test set log-likelihood.

Here is the code to run this experiment and collect the results:
```python
from sklearn.mixture import GaussianMixture
from tqdm import tqdm
from datetime import datetime
from itertools import product

# Get our monthly returns
returns = monthly.pct_change()

# Devolatise the returns
std = returns.ewm(halflife=6, min_periods=12).std()
std = std.shift(1)
devoled = (returns / std).dropna()

# Split into train and test
cutoff = datetime(2022, 1, 1)
train = devoled.loc[:cutoff]
test = devoled.loc[cutoff:]

# Test different numbers of states
states = [8, 7, 6, 5, 4, 3, 2, 1]

# For each state, do multiple runs
runs = range(100)

# Put the states and runs together for
# looping over.
loops = list(product(states, runs))

# We'll put the results of each
# loop in here for analysis later.
results: list[dict] = []

for num_states, run in tqdm(loops):

    gm = GaussianMixture(
        n_components=num_states,
        n_init=40,
        init_params='random',
    )

    gm.fit(train.values)
    
    results.append({
        'num_states': num_states,
        'run': run,
        'train_score': gm.score(train.values),
        'test_score': gm.score(test.values),
    })

results = pd.DataFrame(results)
```

Plotting the media test score per state gives us:

{{<figure src="images/number_of_states.svg" title="Test set scores." >}}
A GMM is fitted to devolatised monthly returns for different numbers of states. The median test set log-likelihood is shown for each number of states. The highest median test set log-likelihood is achieved with 3 states.
{{</figure>}}

For the rest of this article, we will use 3 states.

# Example

Now that we have monthly returns, we know we should devolatise them and we have decided on the number of states, we can fit a GMM to the returns and investigate the results.

We can fit the model with:

```python
from sklearn.mixture import GaussianMixture

returns = monthly.pct_change().dropna()

# Devolatise the returns
std = returns.ewm(halflife=6, min_periods=12).std()
std = std.shift(1)
devoled = (returns / std).dropna()

model = GaussianMixture(
    n_components=3,
    random_state=42,
    n_init=40,
    init_params='random',
)
model.fit(devoled)
```

The fitted mixing coefficients ($\pi_k$) can be found with `model.weights_` and look like this:

|   State 0 |   State 1 |   State 2 |
|-----------|-----------|-----------|
|    40.21% |    33.04% |    26.75% |

In this case, the model is saying that State 0 is the most common state, followed by State 1 and then State 2.

We can look at the mean vectors ($\boldsymbol{\mu}_k$) with `model.means_`:

|     |   State 0 |   State 1 |   State 2 |
|:----|----------:|----------:|----------:|
| SPY |    59.95% |    27.56% |   -47.29% |
| TLT |    -4.81% |    12.95% |    68.64% |
| GLD |    41.24% |   -35.78% |    48.17% |
| GSG |    35.67% |    -8.04% |   -26.61% |

It is very difficult to just look at these numbers and name the states. However, we already start to get an idea of what might be going on. From looking at the mean vectors, State 0 looks inflationary with positive returns for all hard assets and a negative return for the cash related asset (TLT bonds). State 1 looks more deflationary with a smaller mean for equities, cash now with positive returns and gold & commodities declining in value. State 2 looks like a destressed state with large negative means for equities and commodities but large positive means for bonds and gold which are traditionally flight to safety assets. 

We can also look at the covariance matrices ($\boldsymbol{\Sigma}_k$) with `model.covariances_`. These are a bit more difficult to visualise, but we can look at the annualised standard deviations (the square root of the diagonal elements multiplied by $\sqrt{12}$):

|     |   State 0 |   State 1 |   State 2 |
|:----|----------:|----------:|----------:|
| SPY |      2.37 |      3.31 |      4.56 |
| TLT |      2.74 |      3.20 |      5.19 |
| GLD |      3.56 |      2.05 |      4.93 |
| GSG |      2.36 |      4.32 |      4.45 |


The main thing to notice here is that State 2 has much higher volatilities than the other two states. This is inline with the earlier intuition that State 2 is a destressed state.

The story so far is that we have an inflationary market (State 0), a deflationary market (State 1) and a destressed market (State 2).

The posterior probabilities are good to check for sanity reasons, but they're not easy to interpret on their own. We can get them with:

```python
model.predict_proba(devoled)
```
and they look like:

![](images/posteriors.svg)

We can get a better view of how the different states behave by multiplying the returns by the posterior probablities. To get a frame of each state's returns for a single asset, we can do:

```python
posterior.multiply(returns['SPY'], axis=0)
```

Looking at State 0:

![](images/state_0_returns.svg)

The statistics we looked at for State 0 suggested an inflationary market and this plot confirms that. All of the returns are positive except for TLT which is the cash related asset.

State 1 looks like:

![](images/state_1_returns.svg)

This is also inline with our earlier intuition that state 1 is a deflationary market. Equity returns are lackluster. Cash does better than the inflationary market. Gold and commodities do poorly.

Finally, state 2:

![](images/state_2_returns.svg)

This is the most interesting state. Here we see the flight to safety assets do best relative to equities and commodities. Together with the higher volatilities in this state, we can easily conlcude that this is a market in distress.

The conclusion from this example is that a Gaussian Mixture Model can identify economically meaningful latent market states. The model was able to not just identify up and down martkets, but also periods of distress.

However, once the model is fitted, we are limited to the sample wide set of mixing coefficients $\pi_k$. This means we are not able to predict future returns any better than using the empirical mean and covariance. We need to extend this model to be able to predict future states.

# Covariates

Once we have fitted a GMM to historical returns, we have the following fitted parameters:
1. Mixing coefficients $p(k) = \pi_k$
2. Mean vectors $\boldsymbol{\mu}_k$
3. Covariance matrices $\boldsymbol{\Sigma}_k$

The next thing we want to do is make a prediction about the state at some time $t$. Specifically, we want to predict the probability of being in each state at time $t$. With the current model, the mixing coefficients $\pi_k$ are fixed. This means that our best estimate for probability of being in state $k$ at time $t$ is $\pi_k$.

## Incorporating covariates

Now, let's say we have some extra information that we at time $t$ before $\boldsymbol{r}_t$ is realised. Let's denote it with the vector $\boldsymbol{x}_t$. Now, rather than modelling $p(k)$ we can model $p(k | \boldsymbol{x}_t)$. This means that our mixing coefficients are now time-varying and depend on the information $\boldsymbol{x}_t$.

We're going to model $p(k | \boldsymbol{x}_t)$ as a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) [^Gruen2008]:
$$
p(k|\boldsymbol{x}_t) = \frac{e^{\boldsymbol{\beta}_k^T\boldsymbol{x}_t}}{\sum_j^K e^{\boldsymbol{\beta}\_j^T\boldsymbol{x}\_t}}
$$
where the coefficients $\boldsymbol{\beta}_k$ are parameters to be estimated. 

We initialise the parameters in the same way as before. We set the posterior probabilities randomly and jump to the M-step to set the model's paramters.

**E-step** The expected response works in much the same way as before, we just use  $p(k | \boldsymbol{x}_t)$ intead of $p(k)$:
$$
E[z\_{tk}] = p(k | \boldsymbol{r}_t, \boldsymbol{x}_t) = \frac{p(k|\boldsymbol{x}_t)\mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j^K p(j|\boldsymbol{x}_t)\mathcal{N}(\boldsymbol{r}_t | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
$$

**M-step** The state means and covariance matrices are the same as before:
$$
\begin{aligned}
\boldsymbol{\mu}_k &= \frac{1}{T_k}\sum_t E[z\_{tk}] \boldsymbol{r}_t \\\
\boldsymbol{\Sigma}_k &= \frac{1}{T_k} \sum_t E[z\_{tk}] (\boldsymbol{r}_t - \boldsymbol{\mu}_k)(\boldsymbol{r}_t - \boldsymbol{\mu}_k)^\top
\end{aligned}
$$

To find the logistic regression parameters $\boldsymbol{\beta}_k$, we take the full set log-likelihood and isolate the terms that depend on these parameters:
$$
\mathcal{L}( \boldsymbol{R}, \boldsymbol{Z} | \boldsymbol{\beta}, \boldsymbol{b}) = \sum_t \sum_k E[z\_{tk}] \log \left( p(k|\boldsymbol{x}_t) \right) + \text{const}
$$
expanding out $p(k|\boldsymbol{x}_t)$ gives us:
$$
\mathcal{L}( \boldsymbol{R}, \boldsymbol{Z} | \boldsymbol{\beta}, \boldsymbol{b}) = \sum_t \sum_k E[z\_{tk}] \left( \boldsymbol{\beta}_k^T\boldsymbol{x}_t - \log \left( \sum_j^K e^{\boldsymbol{\beta}\_j^T\boldsymbol{x}\_t} \right) \right) + \text{const}
$$
We'll also add a L2 regularisation term to avoid overfitting:
$$
-\lambda \sum_k \left( ||\boldsymbol{\beta}\_k||^2 \right)
$$

To find the maximum we take the derivative with respect to $\boldsymbol {\beta}_k$:
$$
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\beta}_k} = \sum_t \left( E[z\_{tk}] - p(k|\boldsymbol{x}_t) \right) \boldsymbol{x}_t - 2 \lambda \boldsymbol{\beta}_k
$$
and set it to zero. There is no closed form solution for $\boldsymbol{\beta}_k$, but we can use gradient ascent to find the maximum.

A couple of notes on solving for $\boldsymbol{\beta}_k$:

1. We do not want to regularise the intercept term. We do not include the intercept in the regularisation term.
1. The model has an indeterminancy as we can scale all the coefficents by a constant and get the same probabilities. Intuitively, as all the probabilites must sum to 1, then one of the probabilities is given by the remaining. To avoid this indeterminancy, we fix one of the state's coefficents to 0. This means we are estimating the other states relative to that state.

## Code

The code to fit this model is quite involved. Rather than paste it all here, you can find it in this gist: [link](https://gist.github.com/robolyst/a4ac0a38634673cd25c4879d9c9c86fe).

Some things to note about the implementation:
1. The code creates a class called `CondGaussianMixture` which implements the conditional algorithm described above.
1. The docstring for the class contains instructions on how to exactly match the output of the scikit-learn `GaussianMixture` class for the unconditional GMM.
1. The logistic regression is solved with the `scipy.optimize.minimize` function.
1. The class contains functions to calculate the prior probabilities, the posterior probabilities, the conditional mean, and the conditional covariance.


# Economic analysis

We now have a hidden state model with time-varying mixing coefficients. This means that we can investigate how different variables in $\boldsymbol{x}_t$ affect the probability of being in each state.

## Macro-economic variables

We're going to investigate the use of macro-economic variables as the covariates. Specifically, we're going to use the dataset [FRED-MD](https://www.stlouisfed.org/research/economists/mccracken/fred-databases) [^McCracken2015]. This is a dataset of monthly US macro-economic variables maintained by the Federal Reserve Bank of St. Louis. The dataset is available in monthly vintages. That is, each month we get a new dataset with the latest data and revisions to past data. In this article, we'll just use the latest vintage available at the time of writing.

You can download that CSV file here: [2025-10-md.csv](https://www.stlouisfed.org/-/media/project/frbstl/stlouisfed/research/fred-md/monthly/2025-10-md.csv).

The FRED-MD dataset contains 124 variables once it's cleaned and it goes back to 1959. Variables cover a wide range, some examples are:

**US Treasury yields.** The 10 year yield (GS10), 5 year (GS5) and 1 year (GS1) are included. These yields reflect the market's expectations of future interest rates and economic growth.
![](images/treasury_yields.svg)

**VIX.** The VIX index (VIXCLSx) is a measure of market volatility derived from S&P 500 options prices. It is often referred to as the "fear gauge" as it tends to spike during periods of market stress.
![](images/vix.svg)

**US CPI growth rate.** The CPI (CPIAUCSL) growth rate is a measure of inflation. Here we show the index transformed by taking the 12-step percentage change. This shows the annual inflation rate.
![](images/cpi.svg)

**US money supply.** The M2 money (M2SL) supply is the federal reserves estimate of total liquid assets in the economy. Here we show the index transformed by taking the 12-step percentage change. This shows the annual rate of change in the money supply.
![](images/money_supply.svg)

**Unemployment rate.** The unemployment rate (UNRATE) is a measure of the percentage of people who are unemployed.
![](images/unrate.svg)

**Consumer sentiment index.** The consumer sentiment index (UMCSENTx) is a measure of consumer confidence by the University of Michigan.
![](images/umcsent.svg)

The FRED-MD dataset needs to be lightly cleaned and many of the variables transformed. For example, the CPI index needs to be converted to a growth rate. The dataset includes a transformation code for each variable to help with this which is documented in their paper [^McCracken2015]. The code I used is:

```python
import pandas as pd

fred = pd.read_csv('2025-10-MD.csv')

# The first row contains transformation codes
tcodes = fred.loc[0].drop('sasdate')

# Drop the first row and parse dates
fred = fred.iloc[1:]
fred['sasdate'] = pd.to_datetime(fred['sasdate'])
fred = fred.set_index('sasdate')

# This is the last month without missing data
fred = fred.loc[:'2025-07-01']

# Forward fill missing data
fred = fred.ffill()

# Two of the series have issues
# ACOGNO starts late
# NONBORRES just looks funny
fred = fred.drop(columns=['ACOGNO', 'NONBORRES']).dropna()

# Apply transformations based on tcodes as specified
# in the FRED-MD paper.
for col in fred:
    match tcodes[col]:
        case 2:
            fred[col] = fred[col].diff()
        case 5 | 6:
            fred[col] = fred[col].pct_change()

# If there are any remaining NaNs, drop them.
fred = fred.dropna()
```

## Impact of variables

A multinomial logistic regression model is not very interpretable because the coefficient for state $k$ on variable $x_j$ effects all the other states due to the normalisation. We can still get an idea of the impact of each variable on each state's probability by calculating the average [marginal effects](https://bookdown.org/mike/data_analysis/sec-marginal-effects.html) (AME).

The marginal effect tells us how much the probability of being in state $k$ at time $t$ changes when we change variable $x\_{tj}$ by a small amount. The average marginal effect is just the average of the marginal effects over all samples. We write the "average marginal effect of variable $j$ on state $k$" as:
$$
\begin{aligned}
\text{AME}_{jk}&= \frac{1}{T} \sum_t \frac{\partial p(k|\boldsymbol{x}_t)}{\partial x\_{tj}} \\\
&= \frac{1}{T} \sum_t \left[ p(k|\boldsymbol{x}_t) \left( \beta\_{kj} - \sum_m^K p(m|\boldsymbol{x}_t) \beta\_{mj} \right)\right] \\
\end{aligned}
$$
Note that this is a function of the prior probabilities and the logistic regression coefficients. The class `CondGaussianMixture` has a method which calculates the AME for us:
```python
class CondGaussianMixture:
    ...

    def ame(self, X: np.ndarray) -> np.ndarray:
        """
        Compute average marginal effects for the
        multinomial logistic regression.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The conditioning features.

        Returns
        -------
        ame : ndarray, shape (n_features, n_states)
            Average marginal effect of each
            feature on each state.
        """
```

## Results

We now have all the pieces to get prices, devolatise returns, get economic variables, fit a conditional Gaussian mixture model, check the states make economic sense, and calculate the average marginal effects for each variable.

### Fit the model

We start by prepping the dataset for fitting by a model:

```python
returns = monthly.pct_change().dropna()
fred_md = pd.read_parquet('data/fred_md.parquet')

# Use these to devolatise the returns
std = returns.ewm(halflife=6, min_periods=12).std()
std = std.shift(1)

X = fred_md.copy()
y = returns / std

# Match dates
dates = X.dropna().index.intersection(y.dropna().index)
X = X.loc[dates]
y = y.loc[dates]

# Standardise the features for
# fitting by a ML model.
X = X - X.mean()
X = X / X.std()

# Make sure we have an intercept
X['intercept'] = 1.0
```

And then we fit the model:
```python
from condgmm import CondGaussianMixture

model = CondGaussianMixture(
    random_state=42,  # For reproducibility
    n_components=3,
    intercept_index=-1,  # Last column is intercept
    n_init=40,
    init_params='random',
    # Chosen to minimise fit time, smaller
    # values take longer to fit. Larger
    # values take less time, but limit
    # the accuracy of the logistic regression.
    l2_penalty=0.01,
)

model.fit(X=X.values, y=y.values)
```

### Check the states

When we first fitted a GMM, we found three economically meaningful states. We found an inflationary state, a deflationary state and a distressed state. We can check if the conditional GMM finds similar states by looking at the state means and volatilities.

The means:

|     |   State 0 |   State 1 |   State 2 |
|:----|----------:|----------:|----------:|
|     |   *deflationary* |    *distress* |    *inflationary* |
| SPY |    -1.58% |   -58.67% |    74.09% |
| TLT |    -5.82% |   131.24% |   -19.21% |
| GLD |   -46.68% |    55.27% |    36.56% |
| GSG |   -44.10% |    -5.99% |    40.53% |

The volatilities:

|     |   State 0 |   State 1 |   State 2 |
|:----|----------:|----------:|----------:|
|     |   *deflationary* |    *distress* |    *inflationary* |
| SPY |      3.20 |      4.38 |      2.36 |
| TLT |      3.18 |      3.86 |      2.78 |
| GLD |      2.16 |      4.63 |      3.56 |
| GSG |      3.84 |      4.93 |      2.50 |

State 2 looks like the inflationary state with strong positive returns for all assets except TLT. State 1 looks like the distressed state with large negative returns for equities, strong returns for the flight to safety assets and the highest levels of volatility. State 0 looks like the deflationary state with negative returns for all assets.

You will notice that the ordering of the states is different to before. This is because the states are identified in a different order each time the model is fitted. This is an indeterminancy. The important thing is that the states retain their economic meaning despite the new dependency on economic variables.

### Analyse AME

For each variable in the FRED-MD dataset, we're going to calculate the maximum absolute average marginal effect across the three states. Then, we'll look at the top 10 variables to see if they make economic sense.

The code for the analysis:
```python
ame = pd.DataFrame(
    model.ame(X.values),
    index=X.columns,
)

# We don't care about the intercept here
ame = ame.drop('intercept', axis=0)

# Calculate the score
ame['score'] = ame.abs().max(1)

# Sort
ame = ame.sort_values('score', ascending=False)

# And keep the best 10
ame = ame.head(10)
```
which gives us the following results:

|               |   State 0 |   State 1 |   State 2 |
|:--------------|----------:|----------:|----------:|
|     |   *deflationary* |    *distress* |    *inflationary* |
| S&P 500       |    -3.23% |    -6.73% |     9.96% |
| S&P div yield |     2.20% |     5.55% |    -7.75% |
| OILPRICEx     |    -6.47% |     0.78% |     5.69% |
| GS10          |     0.50% |    -6.18% |     5.67% |
| CES1021000001 |    -4.04% |    -1.60% |     5.64% |
| BAA           |    -0.28% |    -5.30% |     5.59% |
| WPSID61       |     4.75% |     0.74% |    -5.49% |
| AAA           |     2.07% |    -5.45% |     3.37% |
| BUSINVx       |    -0.66% |    -4.58% |     5.24% |
| VIXCLSx       |     2.66% |     2.58% |    -5.24% |

**S&P 500.** The [S&P 500 index](https://fred.stlouisfed.org/series/SP500) level has a strong positive effect on the probability of being in the inflationary state and a strong negative effect on the probability of being in the distressed states. This makes sense as a rising stock market is often associated with economic growth and inflation.

**S&P div yield.** The S&P 500 dividend yield has a strong negative effect on the probability of being in the inflationary state and a strong positive effect on the deflationary and distressed states. This is intuitive as, all else being equal, as prices rice (inflationary) the dividend yield falls. Conversely, in deflationary or distressed states, prices fall and dividend yields rise.

**OILPRICEx.** The [oil price](https://fred.stlouisfed.org/series/OILPRICE) has a strong positive effect on the probability of being in the inflationary state and a strong negative effect on the deflationary state.

**GS10.** The [10 year treasury yield](https://fred.stlouisfed.org/series/GS10) has a strong positive effect on the probability of being in the inflationary state and a strong negative effect on the distressed state. This is intuitive as rising yields are associated with inflation. Similarly, lower yields see more precarious borrowing (leading) or during times of distress investors flock to the safety of treasuries pushing yields down (lagging).

**CES1021000001.** This is the [total number of people employed in the mining sector including jobs in mining, quarry, oil and gas](https://fred.stlouisfed.org/series/CES1021000001). I'm uncertain why this variable has such a strong effect vs the other employment variables.

**AAA** and **BAA.** These are the yields on [AAA](https://fred.stlouisfed.org/series/AAA) and [BAA](https://fred.stlouisfed.org/series/BAA) rated corporate bonds. Both have a strong positive effect on the probability of being in the inflationary state and a strong negative effect on the distressed state. There is probably quite a bit of economics as to why this is. As a leading indicator, falling corporate bond yields may be a sign of under-pricing risk.

**WPSID61.** This is the [producer price index for intermediate materials](https://fred.stlouisfed.org/series/WPSID61), supplies and components. It has a strong positive effect on the deflationary state and a strong negative effect on the inflationary state. This makes sense as rising input prices lower profit margins which can lead to deflationary pressures.

**BUSINVx.** This is the [total business inventories](https://fred.stlouisfed.org/series/BUSINV). It has a strong positive effect on the inflationary state and a strong negative effect on the distressed state. This makes sense as rising inventories relative to sales can be a sign of strong demand which can lead to high profits.

**VIXCLSx.** The [VIX index](https://fred.stlouisfed.org/series/VIXCLS) is a well known indicator of market uncertainty and distress. Unsurprisingly, when the VIX is increasing, there is a lower chance being in the inflationary state.

# Summary

In this article, we have seen how to fit a Gaussian Mixture Model to returns to identify meaningful market regimes. We then extended this model to include covariates which allowed us to have time-varying mixing coefficients. Finally, we applied this model to macro-economic variables to see how they affect the probability of being in each market state.

While the analysis was done in-sample, the results are promising. The model was able to identify economically meaningful states and the macro-economic variables had intuitive effects on the state probabilities.


{{% citation
    id="Bishop2006"
    author="Christopher M. Bishop"
    title="Pattern Recognition and Machine Learning"
    year="2006"
    publication="Springer"
    link="https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf"
%}}

{{% citation
    id="Gruen2008"
    author="Bettina Grt√ºn and Friedrich Leisch"
    title="FlexMix Version 2: Finite Mixtures with Concomitant Variables and Varying and Constant Parameters"
    year="2008"
    publication="Journal of Statistical Software"
    link="https://www.jstatsoft.org/article/view/v028i04"
%}}

{{% citation
    id="Botte2021"
    author="Alex Botte and Doris Bao"
    title="A Machine Learning Approach to Regime Modeling"
    year="2021"
    publication="Two Sigma"
    link="https://www.twosigma.com/articles/a-machine-learning-approach-to-regime-modeling/"
%}}

{{% citation
    id="McCracken2015"
    author="Michael W. McCracken and Serena Ng"
    title="FRED-MD: A Monthly Database for Macroeconomic Research"
    year="2015"
    publication="Federal Reserve Bank of St. Louis Working Paper"
    link="https://doi.org/10.20955/wp.2015.012"
%}}
